{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80cbf830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: textblob in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Requirement already satisfied: wordcloud in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from wordcloud) (1.24.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from wordcloud) (3.7.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (23.1)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dawood md\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install textblob\n",
    "!pip install wordcloud\n",
    "\n",
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word, TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4bd520a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Stars</th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Keshav agarwal</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0 out of 5 stars\\nWatch</td>\n",
       "      <td>17 June 2024</td>\n",
       "      <td>The watch is amazing and I liked it The watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mustaque Ali Ansari</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0 out of 5 stars\\nValue for money and a nice...</td>\n",
       "      <td>3 February 2023</td>\n",
       "      <td>Bought this piece for a very peculiar reason. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I bought This watch and it arrived at the same...</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0 out of 5 stars\\nBest watch ever...</td>\n",
       "      <td>10 April 2024</td>\n",
       "      <td>Bought this piece for a very peculiar reason. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harsha n.</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0 out of 5 stars\\nBest in class ever...</td>\n",
       "      <td>13 May 2024</td>\n",
       "      <td>- The screen has 60 hz refresh rate that reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prasadh Laagad</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0 out of 5 stars\\nLooks Good!</td>\n",
       "      <td>20 June 2024</td>\n",
       "      <td>Rectangular shape, good looks, quality of prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bharathwaj Kuppan</td>\n",
       "      <td>5.00</td>\n",
       "      <td>5.0 out of 5 stars\\nBest Smartwatch in this se...</td>\n",
       "      <td>10 June 2024</td>\n",
       "      <td>I ordered Noise Colorfit Pro Max 5 and it got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subhendu</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.0 out of 5 stars\\nGood product</td>\n",
       "      <td>21 June 2024</td>\n",
       "      <td>Good product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AmzonCust</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0 out of 5 stars\\nThis is a good watch with ...</td>\n",
       "      <td>2 January 2024</td>\n",
       "      <td>This is a good looking all \"bells and whistles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hisham Rahman</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0 out of 5 stars\\n\"ColorFit Pro 5 Max\": It's...</td>\n",
       "      <td>12 June 2024</td>\n",
       "      <td>Product is good. Nothing much to complain.Howe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Abhinav</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.0 out of 5 stars\\nSenseless sensors</td>\n",
       "      <td>20 June 2024</td>\n",
       "      <td>When you will connect your smartwatch on the  ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name  Stars                                              Title             Date                                        Description\n",
       "0                                     Keshav agarwal   5.00                          5.0 out of 5 stars\\nWatch     17 June 2024  The watch is amazing and I liked it The watch ...\n",
       "1                                Mustaque Ali Ansari   4.00  4.0 out of 5 stars\\nValue for money and a nice...  3 February 2023  Bought this piece for a very peculiar reason. ...\n",
       "2  I bought This watch and it arrived at the same...   5.00             5.0 out of 5 stars\\nBest watch ever...    10 April 2024  Bought this piece for a very peculiar reason. ...\n",
       "3                                          Harsha n.   5.00          5.0 out of 5 stars\\nBest in class ever...      13 May 2024  - The screen has 60 hz refresh rate that reall...\n",
       "4                                     Prasadh Laagad   4.00                    4.0 out of 5 stars\\nLooks Good!     20 June 2024  Rectangular shape, good looks, quality of prod...\n",
       "5                                  Bharathwaj Kuppan   5.00  5.0 out of 5 stars\\nBest Smartwatch in this se...     10 June 2024  I ordered Noise Colorfit Pro Max 5 and it got ...\n",
       "6                                           Subhendu   3.00                   3.0 out of 5 stars\\nGood product     21 June 2024                                       Good product\n",
       "7                                          AmzonCust   4.00  4.0 out of 5 stars\\nThis is a good watch with ...   2 January 2024  This is a good looking all \"bells and whistles...\n",
       "8                                      hisham Rahman   4.00  4.0 out of 5 stars\\n\"ColorFit Pro 5 Max\": It's...     12 June 2024  Product is good. Nothing much to complain.Howe...\n",
       "9                                            Abhinav   4.00              4.0 out of 5 stars\\nSenseless sensors     20 June 2024  When you will connect your smartwatch on the  ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews=pd.read_csv(r\"reviews.csv\")\n",
    "df_reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0adae17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "from textblob.np_extractors import ConllExtractor\n",
    "from sklearn.feature_extraction.text import CountVectorizer ,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from cleantext import clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b1e805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The watch is amazing and I liked it The watch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bought this piece for a very peculiar reason. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bought this piece for a very peculiar reason. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- The screen has 60 hz refresh rate that reall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rectangular shape, good looks, quality of prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>It‚Äôs good to looks wear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>The watch is good after 7 day review, nice int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Nice watch, durable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>üòçüòç</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Description\n",
       "0   The watch is amazing and I liked it The watch ...\n",
       "1   Bought this piece for a very peculiar reason. ...\n",
       "2   Bought this piece for a very peculiar reason. ...\n",
       "3   - The screen has 60 hz refresh rate that reall...\n",
       "4   Rectangular shape, good looks, quality of prod...\n",
       "..                                                ...\n",
       "95                            It‚Äôs good to looks wear\n",
       "96  The watch is good after 7 day review, nice int...\n",
       "97                                                NaN\n",
       "98                                Nice watch, durable\n",
       "99                                                 üòçüòç\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(df_reviews['Description'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49d6f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(dataframe, dependent_var):\n",
    "  # Normalizing Case Folding - Uppercase to Lowercase\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "  # Removing Punctuation\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]','')\n",
    "\n",
    "  # Removing Numbers\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d','')\n",
    "\n",
    "  # StopWords\n",
    "  sw = stopwords.words('english')\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "  # Remove Rare Words\n",
    "  temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "  drops = temp_df[temp_df <= 1]\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "  # Lemmatize\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8626225",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text_preprocessing(df, \"Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cce9bdb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    watch amazing liked watch really loud enough s...\n",
       "1    bought piece peculiar reason needed watch remi...\n",
       "2    bought piece peculiar reason needed watch remi...\n",
       "3    screen hz refresh rate really make responsive ...\n",
       "4    good look quality product hardware seems good ...\n",
       "Name: Description, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Description\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a721932b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch amazing liked watch really loud enough s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bought piece peculiar reason needed watch remi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought piece peculiar reason needed watch remi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>screen hz refresh rate really make responsive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good look quality product hardware seems good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>good look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>watch good day review nice interface backup da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>nice watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Description\n",
       "0   watch amazing liked watch really loud enough s...\n",
       "1   bought piece peculiar reason needed watch remi...\n",
       "2   bought piece peculiar reason needed watch remi...\n",
       "3   screen hz refresh rate really make responsive ...\n",
       "4   good look quality product hardware seems good ...\n",
       "..                                                ...\n",
       "95                                          good look\n",
       "96  watch good day review nice interface backup da...\n",
       "97                                                nan\n",
       "98                                         nice watch\n",
       "99                                                   \n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import emoji\n",
    "\n",
    "# Replace emojis with an empty string\n",
    "df['Description'] = df['Description'].apply(lambda s: emoji.replace_emoji(str(s), '') if isinstance(s, str) else '')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "008cabac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch amazing liked watch really loud enough s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bought piece peculiar reason needed watch remi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought piece peculiar reason needed watch remi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>screen hz refresh rate really make responsive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good look quality product hardware seems good ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>good look</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>watch good day review nice interface backup da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>nice watch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Description\n",
       "0   watch amazing liked watch really loud enough s...\n",
       "1   bought piece peculiar reason needed watch remi...\n",
       "2   bought piece peculiar reason needed watch remi...\n",
       "3   screen hz refresh rate really make responsive ...\n",
       "4   good look quality product hardware seems good ...\n",
       "..                                                ...\n",
       "95                                          good look\n",
       "96  watch good day review nice interface backup da...\n",
       "97                                                nan\n",
       "98                                         nice watch\n",
       "99                                                   \n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert all strings in the 'Description' column to lowercase\n",
    "df['Description'] = df['Description'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80480c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "  sia = SentimentIntensityAnalyzer()\n",
    "  dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2ee549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polarity_scores(df, \"Description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebeeccb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>polarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watch amazing liked watch really loud enough s...</td>\n",
       "      <td>0.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bought piece peculiar reason needed watch remi...</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bought piece peculiar reason needed watch remi...</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>screen hz refresh rate really make responsive ...</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>good look quality product hardware seems good ...</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Description  polarity_score\n",
       "0  watch amazing liked watch really loud enough s...            0.98\n",
       "1  bought piece peculiar reason needed watch remi...            0.99\n",
       "2  bought piece peculiar reason needed watch remi...            0.99\n",
       "3  screen hz refresh rate really make responsive ...            0.99\n",
       "4  good look quality product hardware seems good ...            0.83"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7569d",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "924d964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lables\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "    dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "    X = dataframe[dependent_var]\n",
    "    y = dataframe[independent_var]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca1fe4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_label(df, \"Description\", \"sentiment_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a72014c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "def split_dataset(dataframe, X, y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "    return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b734cf4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97314b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_count(train_x, test_x):\n",
    "    # Count Vectors\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "    x_test_count_vectorizer = vectorizer.fit_transform(test_x)\n",
    "\n",
    "    return x_train_count_vectorizer, x_test_count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e603054",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c3228d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "    # TF-IDF word\n",
    "    tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "    x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_word = tf_idf_word_vectorizer.fit_transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_word, x_test_tf_idf_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b547b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3665875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "    # TF-IDF ngram\n",
    "    tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "    x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_ngram, x_test_tf_idf_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45b8534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67f0ae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_TFIDF_chars(train_x, test_x):\n",
    "    # TF-IDF Characters\n",
    "    tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2,3))\n",
    "    x_train_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_chars, x_test_tf_idf_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0500233",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_chars, x_test_tf_idf_chars = create_features_TFIDF_chars(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "660b8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def crate_model_logistic(train_x, test_x):\n",
    "  # Count\n",
    "    x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "    loj_count = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_count = loj_count.fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(loj_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    # TF-IDF Word\n",
    "    x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "    loj_word = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_word = loj_word.fit(x_train_tf_idf_word, train_y)\n",
    "    accuracy_word = cross_val_score(loj_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "    # TF-IDF ngram\n",
    "    x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "    loj_ngram = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_ngram = loj_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "    accuracy_ngram = cross_val_score(loj_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "    # TF-IDF chars\n",
    "\n",
    "    loj_chars = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_chars = loj_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "    accuracy_chars = cross_val_score(loj_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "    return loj_model_count, loj_model_word, loj_model_ngram, loj_model_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca304e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Count Vectors: 0.867\n",
      "Accuracy - TF-IDF Word: 0.867\n",
      "Accuracy TF-IDF ngram: 0.867\n",
      "Accuracy TF-IDF Characters: 0.867\n"
     ]
    }
   ],
   "source": [
    "loj_model_count, loj_model_word, loj_model_ngram, loj_model_chars = crate_model_logistic(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f0ef5596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def crate_model_randomforest(train_x, test_x):\n",
    "    # Count\n",
    "    x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "    rf_count = RandomForestClassifier()\n",
    "    rf_model_count = rf_count.fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(rf_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    # TF-IDF Word\n",
    "    x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "    rf_word = RandomForestClassifier()\n",
    "    rf_model_word = rf_word.fit(x_train_tf_idf_word, train_y)\n",
    "    accuracy_word = cross_val_score(rf_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "    # TF-IDF ngram\n",
    "    x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "    rf_ngram = RandomForestClassifier()\n",
    "    rf_model_ngram = rf_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "    accuracy_ngram = cross_val_score(rf_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "    # TF-IDF chars\n",
    "\n",
    "    rf_chars = RandomForestClassifier()\n",
    "    rf_model_chars = rf_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "    accuracy_chars = cross_val_score(rf_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "    return rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68ea0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Count Vectors: 0.883\n",
      "Accuracy - TF-IDF Word: 0.800\n",
      "Accuracy TF-IDF ngram: 0.267\n",
      "Accuracy TF-IDF Characters: 0.883\n"
     ]
    }
   ],
   "source": [
    "rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars = crate_model_randomforest(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb9b64b",
   "metadata": {},
   "source": [
    "# Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4be0d4cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tuning_randomforest(train_x, test_x):\n",
    "    # Count\n",
    "    x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "    rf_model_count = RandomForestClassifier(random_state=1)\n",
    "    rf_params = {\"max_depth\": [2,5,8, None],\n",
    "                 \"max_features\": [2,5,8, \"auto\"],\n",
    "                 \"n_estimators\": [100,500,1000],\n",
    "                 \"min_samples_split\": [2,5,10]}\n",
    "    rf_best_grid = GridSearchCV(rf_model_count, rf_params, cv=10, n_jobs=-1, verbose=False).fit(x_train_count_vectorizer, train_y)\n",
    "    rf_model_count_final = rf_model_count.set_params(**rf_best_grid.best_params_, random_state=1).fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(rf_model_count_final, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    return rf_model_count_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7627e3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Count Vectors: 0.867\n"
     ]
    }
   ],
   "source": [
    "rf_model_count_final = model_tuning_randomforest(train_x, test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336d080b",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "056756e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_count(train_x, model, new_comment):\n",
    "    new_comment= pd.Series(new_comment)\n",
    "    new_comment = CountVectorizer().fit(train_x).transform(new_comment)\n",
    "    result = model.predict(new_comment)\n",
    "    if result==1:\n",
    "        print(\"Comment is Pozitive\")\n",
    "    else:\n",
    "        print(\"Comment is Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1cec8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment is Pozitive\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression\n",
    "predict_count(train_x, model=loj_model_count, new_comment=\"this product is very good :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dcce238d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment is Pozitive\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "predict_count(train_x, model=rf_model_count, new_comment=\"this product is very bad :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d8df76c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    great product noise app experience also\n",
       "dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Review\n",
    "new_comment=pd.Series(df[\"Description\"].sample(1).values)\n",
    "new_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39e5bc20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    nice product\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Review\n",
    "new_comment=pd.Series(df[\"Description\"].sample(1).values)\n",
    "new_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7975a0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment is Pozitive\n"
     ]
    }
   ],
   "source": [
    "# Sample Review - Random Forest\n",
    "predict_count(train_x, model=rf_model_count, new_comment=new_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27e0cded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Count Vectors: 0.867\n",
      "Accuracy - TF-IDF Word: 0.867\n",
      "Accuracy TF-IDF ngram: 0.867\n",
      "Accuracy TF-IDF Characters: 0.867\n",
      "Comment is Positive\n",
      "Count Vectorizer model saved successfully\n",
      "TF-IDF Word model saved successfully\n",
      "TF-IDF Ngram model saved successfully\n",
      "TF-IDF Chars model saved successfully\n"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word, TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)\n",
    "\n",
    "df_reviews = pd.read_csv(r\"reviews.csv\")\n",
    "df = pd.DataFrame(df_reviews['Description'])\n",
    "\n",
    "# Replace emojis with an empty string\n",
    "df['Description'] = df['Description'].apply(lambda s: emoji.replace_emoji(str(s), '') if isinstance(s, str) else '')\n",
    "\n",
    "def text_preprocessing(dataframe, dependent_var):\n",
    "    # Normalizing Case Folding - Uppercase to Lowercase\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "    # Removing Punctuation\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Removing Numbers\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d', '', regex=True)\n",
    "\n",
    "    # StopWords\n",
    "    sw = stopwords.words('english')\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "    # Remove Rare Words\n",
    "    temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "    drops = temp_df[temp_df <= 1]\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "    # Lemmatize\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = text_preprocessing(df, \"Description\")\n",
    "\n",
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "create_polarity_scores(df, \"Description\")\n",
    "\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "    dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "    X = dataframe[dependent_var]\n",
    "    y = dataframe[independent_var]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = create_label(df, \"Description\", \"sentiment_label\")\n",
    "\n",
    "def split_dataset(dataframe, X, y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)\n",
    "\n",
    "def create_features_count(train_x, test_x):\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "    x_test_count_vectorizer = vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_count_vectorizer, x_test_count_vectorizer, vectorizer\n",
    "\n",
    "x_train_count_vectorizer, x_test_count_vectorizer, count_vectorizer = create_features_count(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "    tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "    x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer\n",
    "\n",
    "x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer = create_features_TFIDF_word(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "    tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "    x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer\n",
    "\n",
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer = create_features_TFIDF_ngram(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_chars(train_x, test_x):\n",
    "    tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3))\n",
    "    x_train_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer\n",
    "\n",
    "x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer = create_features_TFIDF_chars(train_x, test_x)\n",
    "\n",
    "# Logistic Regression\n",
    "def create_model_logistic(train_x, test_x):\n",
    "    # Count\n",
    "    x_train_count_vectorizer, x_test_count_vectorizer, count_vectorizer = create_features_count(train_x, test_x)\n",
    "    loj_count = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_count = loj_count.fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(loj_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    # TF-IDF Word\n",
    "    x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer = create_features_TFIDF_word(train_x, test_x)\n",
    "    loj_word = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_word = loj_word.fit(x_train_tf_idf_word, train_y)\n",
    "    accuracy_word = cross_val_score(loj_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "    # TF-IDF ngram\n",
    "    x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer = create_features_TFIDF_ngram(train_x, test_x)\n",
    "    loj_ngram = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_ngram = loj_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "    accuracy_ngram = cross_val_score(loj_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "    # TF-IDF chars\n",
    "    x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer = create_features_TFIDF_chars(train_x, test_x)\n",
    "    loj_chars = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "    loj_model_chars = loj_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "    accuracy_chars = cross_val_score(loj_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "    return loj_model_count, loj_model_word, loj_model_ngram, loj_model_chars\n",
    "    \n",
    "loj_model_count, loj_model_word, loj_model_ngram, loj_model_chars = create_model_logistic(train_x, test_x)\n",
    "    \n",
    "def predict_count(model, vectorizer, new_comment):\n",
    "    new_comment = pd.Series(new_comment)\n",
    "    new_comment = vectorizer.transform(new_comment)\n",
    "    result = model.predict(new_comment)\n",
    "    if result == 1:\n",
    "        print(\"Comment is Positive\")\n",
    "    else:\n",
    "        print(\"Comment is Negative\")\n",
    "\n",
    "# Example usage\n",
    "predict_count(model=loj_model_count, vectorizer=count_vectorizer, new_comment=\"this product is very good :)\")\n",
    "\n",
    "# Save the models to separate pickle files\n",
    "with open('model_count.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_count, file)\n",
    "print(\"Count Vectorizer model saved successfully\")\n",
    "\n",
    "with open('model_word.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_word, file)\n",
    "print(\"TF-IDF Word model saved successfully\")\n",
    "\n",
    "with open('model_ngram.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_ngram, file)\n",
    "print(\"TF-IDF Ngram model saved successfully\")\n",
    "\n",
    "with open('model_chars.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_chars, file)\n",
    "print(\"TF-IDF Chars model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba70074",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 153\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy TF-IDF Characters: \u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m accuracy_chars)\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars\n\u001b[1;32m--> 153\u001b[0m rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars \u001b[38;5;241m=\u001b[39m crate_model_randomforest(train_x, test_x)\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_count\u001b[39m(model, vectorizer, new_comment):\n\u001b[0;32m    157\u001b[0m     new_comment \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(new_comment)\n",
      "Cell \u001b[1;32mIn[2], line 124\u001b[0m, in \u001b[0;36mcrate_model_randomforest\u001b[1;34m(train_x, test_x)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcrate_model_randomforest\u001b[39m(train_x, test_x):\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Count\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m     x_train_count_vectorizer, x_test_count_vectorizer \u001b[38;5;241m=\u001b[39m create_features_count(train_x, test_x)\n\u001b[0;32m    125\u001b[0m     rf_count \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m    126\u001b[0m     rf_model_count \u001b[38;5;241m=\u001b[39m rf_count\u001b[38;5;241m.\u001b[39mfit(x_train_count_vectorizer, train_y)\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word, TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)\n",
    "\n",
    "df_reviews = pd.read_csv(r\"reviews.csv\")\n",
    "df = pd.DataFrame(df_reviews['Description'])\n",
    "\n",
    "# Replace emojis with an empty string\n",
    "df['Description'] = df['Description'].apply(lambda s: emoji.replace_emoji(str(s), '') if isinstance(s, str) else '')\n",
    "\n",
    "def text_preprocessing(dataframe, dependent_var):\n",
    "    # Normalizing Case Folding - Uppercase to Lowercase\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "    # Removing Punctuation\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Removing Numbers\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d', '', regex=True)\n",
    "\n",
    "    # StopWords\n",
    "    sw = stopwords.words('english')\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "    # Remove Rare Words\n",
    "    temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "    drops = temp_df[temp_df <= 1]\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "    # Lemmatize\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = text_preprocessing(df, \"Description\")\n",
    "\n",
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "create_polarity_scores(df, \"Description\")\n",
    "\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "    dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "    X = dataframe[dependent_var]\n",
    "    y = dataframe[independent_var]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = create_label(df, \"Description\", \"sentiment_label\")\n",
    "\n",
    "def split_dataset(dataframe, X, y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)\n",
    "\n",
    "def create_features_count(train_x, test_x):\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "    x_test_count_vectorizer = vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_count_vectorizer, x_test_count_vectorizer, vectorizer\n",
    "\n",
    "x_train_count_vectorizer, x_test_count_vectorizer, count_vectorizer = create_features_count(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "    tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "    x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer\n",
    "\n",
    "x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer = create_features_TFIDF_word(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "    tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "    x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer\n",
    "\n",
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer = create_features_TFIDF_ngram(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_chars(train_x, test_x):\n",
    "    tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3))\n",
    "    x_train_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer\n",
    "\n",
    "x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer = create_features_TFIDF_chars(train_x, test_x)\n",
    "\n",
    "# Random Forest\n",
    "def crate_model_randomforest(train_x, test_x):\n",
    "    # Count\n",
    "    x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "    rf_count = RandomForestClassifier()\n",
    "    rf_model_count = rf_count.fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(rf_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    # TF-IDF Word\n",
    "    x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "    rf_word = RandomForestClassifier()\n",
    "    rf_model_word = rf_word.fit(x_train_tf_idf_word, train_y)\n",
    "    accuracy_word = cross_val_score(rf_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "    # TF-IDF ngram\n",
    "    x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "    rf_ngram = RandomForestClassifier()\n",
    "    rf_model_ngram = rf_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "    accuracy_ngram = cross_val_score(rf_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "    # TF-IDF chars\n",
    "\n",
    "    rf_chars = RandomForestClassifier()\n",
    "    rf_model_chars = rf_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "    accuracy_chars = cross_val_score(rf_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "    return rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars\n",
    "\n",
    "rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars = crate_model_randomforest(train_x, test_x)\n",
    "\n",
    "    \n",
    "def predict_count(model, vectorizer, new_comment):\n",
    "    new_comment = pd.Series(new_comment)\n",
    "    new_comment = vectorizer.transform(new_comment)\n",
    "    result = model.predict(new_comment)\n",
    "    if result == 1:\n",
    "        print(\"Comment is Positive\")\n",
    "    else:\n",
    "        print(\"Comment is Negative\")\n",
    "\n",
    "# Example usage\n",
    "predict_count(model=loj_model_count, vectorizer=count_vectorizer, new_comment=\"this product is very good :)\")\n",
    "\n",
    "# Save the models to separate pickle files\n",
    "with open('model_count.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_count, file)\n",
    "print(\"Count Vectorizer model saved successfully\")\n",
    "\n",
    "with open('model_word.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_word, file)\n",
    "print(\"TF-IDF Word model saved successfully\")\n",
    "\n",
    "with open('model_ngram.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_ngram, file)\n",
    "print(\"TF-IDF Ngram model saved successfully\")\n",
    "\n",
    "with open('model_chars.pkl', 'wb') as file:\n",
    "    pickle.dump(loj_model_chars, file)\n",
    "print(\"TF-IDF Chars model saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbd8fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Count Vectors: 0.917\n",
      "Accuracy - TF-IDF Word: 0.900\n",
      "Accuracy TF-IDF ngram: 0.867\n",
      "Accuracy TF-IDF Characters: 0.850\n",
      "Comment is Positive\n",
      "Count Vectorizer model saved successfully\n",
      "TF-IDF Word model saved successfully\n",
      "TF-IDF Ngram model saved successfully\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "write to closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 177\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF Ngram model saved successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrf_model_chars.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_4:\n\u001b[1;32m--> 177\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(rf_model_chars, file)\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTF-IDF Chars model saved successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount_vectorizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m vec_1:\n",
      "\u001b[1;31mValueError\u001b[0m: write to closed file"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)\n",
    "\n",
    "# Load data\n",
    "df_reviews = pd.read_csv(r\"reviews.csv\")\n",
    "df = pd.DataFrame(df_reviews['Description'])\n",
    "\n",
    "# Replace emojis with an empty string\n",
    "df['Description'] = df['Description'].apply(lambda s: emoji.replace_emoji(str(s), '') if isinstance(s, str) else '')\n",
    "\n",
    "def text_preprocessing(dataframe, dependent_var):\n",
    "    # Normalizing Case Folding - Uppercase to Lowercase\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "    # Removing Punctuation\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Removing Numbers\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d', '', regex=True)\n",
    "\n",
    "    # StopWords\n",
    "    sw = stopwords.words('english')\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "    # Remove Rare Words\n",
    "    temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "    drops = temp_df[temp_df <= 1]\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "    # Lemmatize\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = text_preprocessing(df, \"Description\")\n",
    "\n",
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "create_polarity_scores(df, \"Description\")\n",
    "\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "    dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "    X = dataframe[dependent_var]\n",
    "    y = dataframe[independent_var]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = create_label(df, \"Description\", \"sentiment_label\")\n",
    "\n",
    "def split_dataset(dataframe, X, y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)\n",
    "\n",
    "def create_features_count(train_x, test_x):\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "    x_test_count_vectorizer = vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_count_vectorizer, x_test_count_vectorizer, vectorizer\n",
    "\n",
    "x_train_count_vectorizer, x_test_count_vectorizer, count_vectorizer = create_features_count(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "    tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "    x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer\n",
    "\n",
    "x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer = create_features_TFIDF_word(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "    tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "    x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer\n",
    "\n",
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer = create_features_TFIDF_ngram(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_chars(train_x, test_x):\n",
    "    tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3))\n",
    "    x_train_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer\n",
    "\n",
    "x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer = create_features_TFIDF_chars(train_x, test_x)\n",
    "\n",
    "# Random Forest\n",
    "def create_model_randomforest(train_x, test_x):\n",
    "    # Count\n",
    "    rf_count = RandomForestClassifier()\n",
    "    rf_model_count = rf_count.fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(rf_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    # TF-IDF Word\n",
    "    rf_word = RandomForestClassifier()\n",
    "    rf_model_word = rf_word.fit(x_train_tf_idf_word, train_y)\n",
    "    accuracy_word = cross_val_score(rf_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "    # TF-IDF ngram\n",
    "    rf_ngram = RandomForestClassifier()\n",
    "    rf_model_ngram = rf_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "    accuracy_ngram = cross_val_score(rf_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "    # TF-IDF chars\n",
    "    rf_chars = RandomForestClassifier()\n",
    "    rf_model_chars = rf_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "    accuracy_chars = cross_val_score(rf_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "    return rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars\n",
    "\n",
    "rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars = create_model_randomforest(train_x, test_x)\n",
    "\n",
    "def predict_sentiment(model, vectorizer, new_comment):\n",
    "    new_comment_series = pd.Series(new_comment)\n",
    "    new_comment_transformed = vectorizer.transform(new_comment_series)\n",
    "    result = model.predict(new_comment_transformed)\n",
    "    if result == 1:\n",
    "        print(\"Comment is Positive\")\n",
    "    else:\n",
    "        print(\"Comment is Negative\")\n",
    "\n",
    "# Example usage\n",
    "predict_sentiment(model=rf_model_count, vectorizer=count_vectorizer, new_comment=\"this product is very good :)\")\n",
    "\n",
    "# Save the models to separate pickle files\n",
    "with open('rf_model_count.pkl', 'wb') as file_1:\n",
    "    pickle.dump(rf_model_count, file_1)\n",
    "print(\"Count Vectorizer model saved successfully\")\n",
    "\n",
    "with open('rf_model_word.pkl', 'wb') as file_2:\n",
    "    pickle.dump(rf_model_word, file_2)\n",
    "print(\"TF-IDF Word model saved successfully\")\n",
    "\n",
    "with open('rf_model_ngram.pkl', 'wb') as file_3:\n",
    "    pickle.dump(rf_model_ngram, file_3)\n",
    "print(\"TF-IDF Ngram model saved successfully\")\n",
    "\n",
    "with open('rf_model_chars.pkl', 'wb') as file_4:\n",
    "    pickle.dump(rf_model_chars, file)\n",
    "print(\"TF-IDF Chars model saved successfully\")\n",
    "\n",
    "with open('count_vectorizer.pkl', 'wb') as vec_1:\n",
    "    pickle.dump(count_vectorizer, vec_1)\n",
    "print(\"Count Vectorizer saved successfully\")\n",
    "\n",
    "with open('tfidf_word_vectorizer.pkl', 'wb') as vec_2:\n",
    "    pickle.dump(tf_idf_word_vectorizer, vec_2)\n",
    "print(\"TF-IDF Word Vectorizer saved successfully\")\n",
    "\n",
    "with open('tfidf_ngram_vectorizer.pkl', 'wb') as vec_3:\n",
    "    pickle.dump(tf_idf_ngram_vectorizer, vec_3)\n",
    "print(\"TF-IDF Ngram Vectorizer saved successfully\")\n",
    "\n",
    "with open('tfidf_chars_vectorizer.pkl', 'wb') as vec_4:\n",
    "    pickle.dump(tf_idf_chars_vectorizer, vec_4)\n",
    "print(\"TF-IDF Chars Vectorizer saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a831ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Dawood\n",
      "[nltk_data]     MD\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - Count Vectors: 0.883\n",
      "Accuracy - TF-IDF Word: 0.867\n",
      "Accuracy TF-IDF ngram: 0.867\n",
      "Accuracy TF-IDF Characters: 0.883\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "import emoji\n",
    "import pickle\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)\n",
    "\n",
    "# Load data\n",
    "df_reviews = pd.read_csv(r\"reviews.csv\")\n",
    "df = pd.DataFrame(df_reviews['Description'])\n",
    "\n",
    "# Replace emojis with an empty string\n",
    "df['Description'] = df['Description'].apply(lambda s: emoji.replace_emoji(str(s), '') if isinstance(s, str) else '')\n",
    "\n",
    "def text_preprocessing(dataframe, dependent_var):\n",
    "    # Normalizing Case Folding - Uppercase to Lowercase\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "    # Removing Punctuation\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]', '', regex=True)\n",
    "\n",
    "    # Removing Numbers\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d', '', regex=True)\n",
    "\n",
    "    # StopWords\n",
    "    sw = stopwords.words('english')\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "    # Remove Rare Words\n",
    "    temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "    drops = temp_df[temp_df <= 1]\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "    # Lemmatize\n",
    "    dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "df = text_preprocessing(df, \"Description\")\n",
    "\n",
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "create_polarity_scores(df, \"Description\")\n",
    "\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "    dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "    X = dataframe[dependent_var]\n",
    "    y = dataframe[independent_var]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = create_label(df, \"Description\", \"sentiment_label\")\n",
    "\n",
    "def split_dataset(dataframe, X, y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)\n",
    "\n",
    "def create_features_count(train_x, test_x):\n",
    "    vectorizer = CountVectorizer()\n",
    "    x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "    x_test_count_vectorizer = vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_count_vectorizer, x_test_count_vectorizer, vectorizer\n",
    "\n",
    "x_train_count_vectorizer, x_test_count_vectorizer, count_vectorizer = create_features_count(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "    tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "    x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_word = tf_idf_word_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer\n",
    "\n",
    "x_train_tf_idf_word, x_test_tf_idf_word, tf_idf_word_vectorizer = create_features_TFIDF_word(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "    tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3))\n",
    "    x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer\n",
    "\n",
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram, tf_idf_ngram_vectorizer = create_features_TFIDF_ngram(train_x, test_x)\n",
    "\n",
    "def create_features_TFIDF_chars(train_x, test_x):\n",
    "    tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 3))\n",
    "    x_train_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(train_x)\n",
    "    x_test_tf_idf_chars = tf_idf_chars_vectorizer.transform(test_x)\n",
    "\n",
    "    return x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer\n",
    "\n",
    "x_train_tf_idf_chars, x_test_tf_idf_chars, tf_idf_chars_vectorizer = create_features_TFIDF_chars(train_x, test_x)\n",
    "\n",
    "# Random Forest\n",
    "def create_model_randomforest(train_x, test_x):\n",
    "    # Count\n",
    "    rf_count = RandomForestClassifier()\n",
    "    rf_model_count = rf_count.fit(x_train_count_vectorizer, train_y)\n",
    "    accuracy_count = cross_val_score(rf_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "    # TF-IDF Word\n",
    "    rf_word = RandomForestClassifier()\n",
    "    rf_model_word = rf_word.fit(x_train_tf_idf_word, train_y)\n",
    "    accuracy_word = cross_val_score(rf_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "    print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "    # TF-IDF ngram\n",
    "    rf_ngram = RandomForestClassifier()\n",
    "    rf_model_ngram = rf_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "    accuracy_ngram = cross_val_score(rf_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "    # TF-IDF chars\n",
    "    rf_chars = RandomForestClassifier()\n",
    "    rf_model_chars = rf_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "    accuracy_chars = cross_val_score(rf_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "    print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "    return rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars\n",
    "\n",
    "rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars = create_model_randomforest(train_x, test_x)\n",
    "\n",
    "def predict_sentiment(model, vectorizer, new_comment):\n",
    "    new_comment_series = pd.Series(new_comment)\n",
    "    new_comment_transformed = vectorizer.transform(new_comment_series)\n",
    "    result = model.predict(new_comment_transformed)\n",
    "    if result == 1:\n",
    "        print(\"Comment is Positive\")\n",
    "    else:\n",
    "        print(\"Comment is Negative\")\n",
    "        \n",
    "# Assuming you have already trained the models and vectorizers\n",
    "# Save each vectorizer and model separately\n",
    "with open('count_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(count_vectorizer, f)\n",
    "    \n",
    "with open('rf_model_count.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model_count, f)\n",
    "\n",
    "with open('tfidf_word_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tf_idf_word_vectorizer, f)\n",
    "with open('rf_model_word.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model_word, f)\n",
    "\n",
    "with open('tfidf_ngram_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tf_idf_ngram_vectorizer, f)\n",
    "with open('rf_model_ngram.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model_ngram, f)\n",
    "\n",
    "with open('tfidf_chars_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tf_idf_chars_vectorizer, f)\n",
    "with open('rf_model_chars.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model_chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d291b80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
